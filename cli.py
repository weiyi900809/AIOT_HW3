import argparse
import sys
from phishing_model import PhishingDetector
from visualization import Visualizer
import numpy as np

def main():
    parser = argparse.ArgumentParser(
        description='é‡£é­šéƒµä»¶æª¢æ¸¬ç³»çµ± - CLI ä»‹é¢',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ç¯„ä¾‹:
  # è¨“ç·´æ¨¡å‹
  python cli.py --train

  # é€²è¡Œé æ¸¬
  python cli.py --predict --input test.csv

  # é¡¯ç¤ºå®Œæ•´å ±å‘Š
  python cli.py --report

  # åƒ…é æ¸¬ï¼ˆä½¿ç”¨å·²è¨“ç·´æ¨¡å‹ï¼‰
  python cli.py --predict-only --input test.csv
        '''
    )
    
    parser.add_argument('--train', action='store_true', 
                       help='è¨“ç·´æ–°æ¨¡å‹')
    parser.add_argument('--predict', action='store_true',
                       help='è¨“ç·´ä¸¦é æ¸¬')
    parser.add_argument('--predict-only', action='store_true',
                       help='ä½¿ç”¨å·²è¨“ç·´æ¨¡å‹é€²è¡Œé æ¸¬')
    parser.add_argument('--input', type=str,
                       help='è¼¸å…¥è³‡æ–™æª”æ¡ˆè·¯å¾‘')
    parser.add_argument('--report', action='store_true',
                       help='é¡¯ç¤ºå®Œæ•´è©•ä¼°å ±å‘Š')
    parser.add_argument('--data', type=str, default='phishing_dataset.csv',
                       help='è¨“ç·´è³‡æ–™é›†è·¯å¾‘ (é è¨­: phishing_dataset.csv)')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–
    detector = PhishingDetector()
    visualizer = Visualizer()
    
    if args.train:
        print("=" * 60)
        print("ğŸ”§ é‡£é­šéƒµä»¶æª¢æ¸¬ç³»çµ± - è¨“ç·´æ¨¡å¼")
        print("=" * 60)
        
        # è¼‰å…¥è³‡æ–™
        X, y = detector.load_data(args.data)
        
        # è³‡æ–™å“è³ªæª¢æŸ¥
        detector.check_data_quality(X, y)
        
        # å‰è™•ç†
        X_train, X_test, y_train, y_test = detector.preprocess_data(X, y)
        
        # è¨“ç·´
        detector.train(X_train, y_train)
        
        # è©•ä¼°
        metrics = detector.evaluate(X_test, y_test)
        
        # è¦–è¦ºåŒ–
        print("\\nğŸ“Š ç”Ÿæˆè¦–è¦ºåŒ–åœ–è¡¨...")
        visualizer.plot_data_distribution(X, y)
        visualizer.plot_confusion_matrix(y_test, metrics['y_pred'])
        visualizer.plot_roc_curve(y_test, metrics['y_pred_proba'])
        visualizer.plot_model_metrics(metrics)
        visualizer.plot_feature_importance(detector.model.coef_)
        
        # ä¿å­˜æ¨¡å‹
        detector.save_model()
        
        print("\\nâœ… è¨“ç·´å®Œæˆï¼")
    
    elif args.predict:
        if not args.input:
            print("âŒ éŒ¯èª¤: é æ¸¬æ¨¡å¼éœ€è¦æŒ‡å®š --input æª”æ¡ˆ")
            sys.exit(1)
        
        print("=" * 60)
        print("ğŸ”® é‡£é­šéƒµä»¶æª¢æ¸¬ç³»çµ± - è¨“ç·´ä¸¦é æ¸¬æ¨¡å¼")
        print("=" * 60)
        
        # è¼‰å…¥è³‡æ–™
        X, y = detector.load_data(args.data)
        detector.check_data_quality(X, y)
        
        # å‰è™•ç†
        X_train, X_test, y_train, y_test = detector.preprocess_data(X, y)
        
        # è¨“ç·´
        detector.train(X_train, y_train)
        metrics = detector.evaluate(X_test, y_test)
        
        # è¼‰å…¥é æ¸¬è³‡æ–™
        print(f"\\nğŸ“‚ è¼‰å…¥é æ¸¬è³‡æ–™: {args.input}")
        try:
            test_data = np.genfromtxt(args.input, delimiter=',', dtype=np.int32)
            if test_data.ndim == 1:
                test_data = test_data.reshape(1, -1)
            
            # é æ¸¬
            predictions, probabilities = detector.predict(test_data)
            
            print(f"\\nğŸ“Š é æ¸¬çµæœ:")
            print("-" * 60)
            for i, (pred, prob) in enumerate(zip(predictions, probabilities)):
                label = "é‡£é­šéƒµä»¶ âš ï¸" if pred == 1 else "åˆæ³•éƒµä»¶ âœ“"
                confidence = max(prob) * 100
                print(f"æ¨£æœ¬ {i+1}: {label} (ä¿¡å¿ƒåº¦: {confidence:.2f}%)")
            
            # ä¿å­˜æ¨¡å‹
            detector.save_model()
            
        except Exception as e:
            print(f"âŒ é æ¸¬å¤±æ•—: {e}")
            sys.exit(1)
    
    elif args.predict_only:
        if not args.input:
            print("âŒ éŒ¯èª¤: éœ€è¦æŒ‡å®š --input æª”æ¡ˆ")
            sys.exit(1)
        
        print("=" * 60)
        print("ğŸ”® é‡£é­šéƒµä»¶æª¢æ¸¬ç³»çµ± - é æ¸¬æ¨¡å¼")
        print("=" * 60)
        
        try:
            # è¼‰å…¥å·²è¨“ç·´æ¨¡å‹
            detector.load_model()
            
            # è¼‰å…¥é æ¸¬è³‡æ–™
            print(f"ğŸ“‚ è¼‰å…¥é æ¸¬è³‡æ–™: {args.input}")
            test_data = np.genfromtxt(args.input, delimiter=',', dtype=np.int32)
            if test_data.ndim == 1:
                test_data = test_data.reshape(1, -1)
            
            # é æ¸¬
            predictions, probabilities = detector.predict(test_data)
            
            print(f"\\nğŸ“Š é æ¸¬çµæœ:")
            print("-" * 60)
            for i, (pred, prob) in enumerate(zip(predictions, probabilities)):
                label = "é‡£é­šéƒµä»¶ âš ï¸" if pred == 1 else "åˆæ³•éƒµä»¶ âœ“"
                confidence = max(prob) * 100
                print(f"æ¨£æœ¬ {i+1}: {label} (ä¿¡å¿ƒåº¦: {confidence:.2f}%)")
                
        except Exception as e:
            print(f"âŒ é æ¸¬å¤±æ•—: {e}")
            sys.exit(1)
    
    elif args.report:
        print("=" * 60)
        print("ğŸ“‹ é‡£é­šéƒµä»¶æª¢æ¸¬ç³»çµ± - å®Œæ•´å ±å‘Š")
        print("=" * 60)
        
        try:
            # è¼‰å…¥è³‡æ–™
            X, y = detector.load_data(args.data)
            
            # è³‡æ–™å“è³ªæª¢æŸ¥
            quality = detector.check_data_quality(X, y)
            
            # å‰è™•ç†
            X_train, X_test, y_train, y_test = detector.preprocess_data(X, y)
            
            # è¨“ç·´
            detector.train(X_train, y_train)
            
            # è©•ä¼°
            metrics = detector.evaluate(X_test, y_test)
            
            # é¡¯ç¤ºçµ±è¨ˆä¿¡æ¯
            print("\\n" + "=" * 60)
            print("ğŸ“ˆ è©³ç´°çµ±è¨ˆä¿¡æ¯")
            print("=" * 60)
            print(f"\\nè³‡æ–™é›†çµ±è¨ˆ:")
            print(f"  - è¨“ç·´é›†å¤§å°: {X_train.shape[0]}")
            print(f"  - æ¸¬è©¦é›†å¤§å°: {X_test.shape[0]}")
            print(f"  - ç‰¹å¾µæ•¸é‡: {X_train.shape[1]}")
            print(f"  - é¡åˆ¥ä¸å¹³è¡¡æ¯”ä¾‹: {quality['imbalance_ratio']:.2f}:1")
            
            print(f"\\næ¨¡å‹æ€§èƒ½:")
            print(f"  - æº–ç¢ºåº¦: {metrics['accuracy']:.4f}")
            print(f"  - ç²¾åº¦: {metrics['precision']:.4f}")
            print(f"  - å¬å›ç‡: {metrics['recall']:.4f}")
            print(f"  - F1 åˆ†æ•¸: {metrics['f1']:.4f}")
            print(f"  - ROC-AUC: {metrics['roc_auc']:.4f}")
            
            if 'cv_scores' in metrics:
                cv_scores = metrics['cv_scores']
                print(f"\\näº¤å‰é©—è­‰çµæœ:")
                print(f"  - å‡å€¼: {cv_scores.mean():.4f}")
                print(f"  - æ¨™æº–å·®: {cv_scores.std():.4f}")
                print(f"  - æœ€å°å€¼: {cv_scores.min():.4f}")
                print(f"  - æœ€å¤§å€¼: {cv_scores.max():.4f}")
            
            # ä¿å­˜æ¨¡å‹
            detector.save_model()
            
        except Exception as e:
            print(f"âŒ å ±å‘Šç”Ÿæˆå¤±æ•—: {e}")
            sys.exit(1)
    
    else:
        parser.print_help()

if __name__ == '__main__':
    main()
