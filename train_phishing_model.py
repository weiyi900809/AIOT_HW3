#!/usr/bin/env python3
"""
Logistic Regression Phishing Detector - è¨“ç·´æ¨¡çµ„
åŒ…å«å®Œæ•´çš„å‰è™•ç†æ­¥é©Ÿã€è³‡æ–™æ¢ç´¢ã€ç‰¹å¾µåˆ†æå’Œæ¨¡å‹è©•ä¼°
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                             f1_score, confusion_matrix, classification_report, roc_auc_score, roc_curve)
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
import warnings
import joblib
warnings.simplefilter('ignore')

# è¨­å®šåœ–è¡¨é¢¨æ ¼
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 6)

class PhishingDetector:
    """é‡£é­šç¶²ç«™åµæ¸¬é¡"""
    
    def __init__(self, data_path='./phishing_dataset.csv'):
        self.data_path = data_path
        self.model = None
        self.scaler = None
        self.metrics = {}
        self.feature_names = None
        
    def load_data(self):
        """è¼‰å…¥è³‡æ–™é›†"""
        print("=" * 70)
        print("ğŸ“Š STEP 1: è³‡æ–™è¼‰å…¥")
        print("=" * 70)
        
        self.dataset = np.genfromtxt(self.data_path, delimiter=',', dtype=np.int32)
        self.samples = self.dataset[:, :-1]
        self.targets = self.dataset[:, -1]
        
        print(f"âœ“ è³‡æ–™é›†å¤§å°: {self.dataset.shape}")
        print(f"âœ“ æ¨£æœ¬æ•¸: {self.samples.shape[0]}")
        print(f"âœ“ ç‰¹å¾µæ•¸: {self.samples.shape[1]}")
        print(f"âœ“ é¡åˆ¥åˆ†ä½ˆ:")
        unique, counts = np.unique(self.targets, return_counts=True)
        for label, count in zip(unique, counts):
            percentage = (count / len(self.targets)) * 100
            print(f"  - é¡åˆ¥ {label}: {count} ç­† ({percentage:.2f}%)")
    
    def explore_data(self):
        """è³‡æ–™æ¢ç´¢èˆ‡çµ±è¨ˆåˆ†æ"""
        print("\n" + "=" * 70)
        print("ğŸ” STEP 2: è³‡æ–™æ¢ç´¢èˆ‡çµ±è¨ˆåˆ†æ")
        print("=" * 70)
        
        print(f"\nâœ“ ç‰¹å¾µçµ±è¨ˆ:")
        print(f"  - æœ€å°å€¼: {self.samples.min()}")
        print(f"  - æœ€å¤§å€¼: {self.samples.max()}")
        print(f"  - å¹³å‡å€¼: {self.samples.mean():.4f}")
        print(f"  - æ¨™æº–å·®: {self.samples.std():.4f}")
        print(f"  - ç¼ºå¤±å€¼: {np.isnan(self.samples).sum()}")
        
        # ç¹ªè£½ç‰¹å¾µåˆ†ä½ˆ
        fig, axes = plt.subplots(2, 3, figsize=(15, 8))
        fig.suptitle('å‰ 6 å€‹ç‰¹å¾µçš„åˆ†ä½ˆ', fontsize=14, fontweight='bold')
        
        for idx in range(min(6, self.samples.shape[1])):
            ax = axes[idx // 3, idx % 3]
            ax.hist(self.samples[:, idx], bins=30, edgecolor='black', alpha=0.7, color='steelblue')
            ax.set_title(f'ç‰¹å¾µ {idx}')
            ax.set_xlabel('å€¼')
            ax.set_ylabel('é »ç‡')
        
        plt.tight_layout()
        plt.savefig('01_feature_distribution.png', dpi=300, bbox_inches='tight')
        print(f"\nâœ“ å·²ä¿å­˜: 01_feature_distribution.png")
        plt.close()
    
    def preprocess_data(self):
        """è³‡æ–™å‰è™•ç†"""
        print("\n" + "=" * 70)
        print("ğŸ”§ STEP 3: è³‡æ–™å‰è™•ç†")
        print("=" * 70)
        
        # æª¢æŸ¥ç•°å¸¸å€¼
        print(f"\nâœ“ ç•°å¸¸å€¼æª¢æ¸¬ (ä½¿ç”¨ IQR æ–¹æ³•):")
        Q1 = np.percentile(self.samples, 25, axis=0)
        Q3 = np.percentile(self.samples, 75, axis=0)
        IQR = Q3 - Q1
        outliers_mask = ((self.samples < (Q1 - 1.5 * IQR)) | (self.samples > (Q3 + 1.5 * IQR)))
        outlier_count = outliers_mask.sum()
        print(f"  - ç•°å¸¸å€¼æ•¸é‡: {outlier_count} ({(outlier_count/(self.samples.size))*100:.2f}%)")
        
        # æ¨™æº–åŒ–ç‰¹å¾µ
        print(f"\nâœ“ ç‰¹å¾µæ¨™æº–åŒ– (StandardScaler)...")
        self.scaler = StandardScaler()
        self.samples_scaled = self.scaler.fit_transform(self.samples)
        print(f"  - æ¨™æº–åŒ–å¾Œå¹³å‡å€¼: {self.samples_scaled.mean():.6f}")
        print(f"  - æ¨™æº–åŒ–å¾Œæ¨™æº–å·®: {self.samples_scaled.std():.6f}")
    
    def split_data(self, test_size=0.2, random_state=42):
        """åˆ†å‰²è¨“ç·´/æ¸¬è©¦é›†"""
        print("\n" + "=" * 70)
        print("âœ‚ï¸  STEP 4: è³‡æ–™åˆ†å‰²")
        print("=" * 70)
        
        self.train_samples, self.test_samples, self.train_targets, self.test_targets = train_test_split(
            self.samples_scaled, self.targets, test_size=test_size, random_state=random_state, stratify=self.targets
        )
        
        print(f"\nâœ“ è¨“ç·´é›†: {self.train_samples.shape[0]} ç­† ({(1-test_size)*100:.0f}%)")
        print(f"âœ“ æ¸¬è©¦é›†: {self.test_samples.shape[0]} ç­† ({test_size*100:.0f}%)")
        print(f"\nâœ“ è¨“ç·´é›†é¡åˆ¥åˆ†ä½ˆ:")
        train_unique, train_counts = np.unique(self.train_targets, return_counts=True)
        for label, count in zip(train_unique, train_counts):
            print(f"  - é¡åˆ¥ {label}: {count} ç­†")
    
    def train_model(self):
        """æ¨¡å‹è¨“ç·´"""
        print("\n" + "=" * 70)
        print("ğŸ¤– STEP 5: æ¨¡å‹è¨“ç·´")
        print("=" * 70)
        
        print(f"\nâœ“ å»ºç«‹ Logistic Regression æ¨¡å‹...")
        self.model = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)
        self.model.fit(self.train_samples, self.train_targets)
        
        print(f"âœ“ æ¨¡å‹è¨“ç·´å®Œæˆ")
        print(f"  - æ¨¡å‹åƒæ•¸: {self.model.get_params()}")
    
    def evaluate_model(self):
        """æ¨¡å‹è©•ä¼°"""
        print("\n" + "=" * 70)
        print("ğŸ“ˆ STEP 6: æ¨¡å‹è©•ä¼°")
        print("=" * 70)
        
        # è¨“ç·´é›†é æ¸¬
        train_pred = self.model.predict(self.train_samples)
        train_pred_proba = self.model.predict_proba(self.train_samples)[:, 1]
        
        # æ¸¬è©¦é›†é æ¸¬
        test_pred = self.model.predict(self.test_samples)
        test_pred_proba = self.model.predict_proba(self.test_samples)[:, 1]
        
        # è¨ˆç®—æŒ‡æ¨™
        print(f"\nâœ“ è¨“ç·´é›†æ€§èƒ½:")
        train_accuracy = accuracy_score(self.train_targets, train_pred)
        train_precision = precision_score(self.train_targets, train_pred)
        train_recall = recall_score(self.train_targets, train_pred)
        train_f1 = f1_score(self.train_targets, train_pred)
        train_auc = roc_auc_score(self.train_targets, train_pred_proba)
        
        print(f"  - æº–ç¢ºç‡ (Accuracy): {train_accuracy:.4f}")
        print(f"  - ç²¾æº–ç‡ (Precision): {train_precision:.4f}")
        print(f"  - å¬å›ç‡ (Recall): {train_recall:.4f}")
        print(f"  - F1 åˆ†æ•¸: {train_f1:.4f}")
        print(f"  - AUC åˆ†æ•¸: {train_auc:.4f}")
        
        print(f"\nâœ“ æ¸¬è©¦é›†æ€§èƒ½:")
        test_accuracy = accuracy_score(self.test_targets, test_pred)
        test_precision = precision_score(self.test_targets, test_pred)
        test_recall = recall_score(self.test_targets, test_pred)
        test_f1 = f1_score(self.test_targets, test_pred)
        test_auc = roc_auc_score(self.test_targets, test_pred_proba)
        
        print(f"  - æº–ç¢ºç‡ (Accuracy): {test_accuracy:.4f}")
        print(f"  - ç²¾æº–ç‡ (Precision): {test_precision:.4f}")
        print(f"  - å¬å›ç‡ (Recall): {test_recall:.4f}")
        print(f"  - F1 åˆ†æ•¸: {test_f1:.4f}")
        print(f"  - AUC åˆ†æ•¸: {test_auc:.4f}")
        
        # äº¤å‰é©—è­‰
        print(f"\nâœ“ 5 æŠ˜äº¤å‰é©—è­‰:")
        cv_scores = cross_val_score(self.model, self.train_samples, self.train_targets, cv=5, scoring='accuracy')
        print(f"  - å¹³å‡ CV æº–ç¢ºç‡: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")
        
        # å„²å­˜æŒ‡æ¨™
        self.metrics = {
            'train_accuracy': train_accuracy, 'train_precision': train_precision,
            'train_recall': train_recall, 'train_f1': train_f1, 'train_auc': train_auc,
            'test_accuracy': test_accuracy, 'test_precision': test_precision,
            'test_recall': test_recall, 'test_f1': test_f1, 'test_auc': test_auc,
            'cv_mean': cv_scores.mean(), 'cv_std': cv_scores.std()
        }
        
        # æ··æ·†çŸ©é™£
        cm = confusion_matrix(self.test_targets, test_pred)
        print(f"\nâœ“ æ··æ·†çŸ©é™£:")
        print(f"  {cm}")
        
        # è¦–è¦ºåŒ–æ··æ·†çŸ©é™£èˆ‡ ROC æ›²ç·š
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
        
        # æ··æ·†çŸ©é™£
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])
        axes[0].set_title('æ··æ·†çŸ©é™£ (æ¸¬è©¦é›†)', fontweight='bold')
        axes[0].set_xlabel('é æ¸¬æ¨™ç±¤')
        axes[0].set_ylabel('çœŸå¯¦æ¨™ç±¤')
        
        # ROC æ›²ç·š
        fpr, tpr, _ = roc_curve(self.test_targets, test_pred_proba)
        axes[1].plot(fpr, tpr, label=f'AUC = {test_auc:.4f}', linewidth=2, color='steelblue')
        axes[1].plot([0, 1], [0, 1], 'k--', linewidth=1)
        axes[1].set_xlabel('False Positive Rate')
        axes[1].set_ylabel('True Positive Rate')
        axes[1].set_title('ROC æ›²ç·š (æ¸¬è©¦é›†)', fontweight='bold')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('02_confusion_matrix_roc.png', dpi=300, bbox_inches='tight')
        print(f"\nâœ“ å·²ä¿å­˜: 02_confusion_matrix_roc.png")
        plt.close()
    
    def save_model(self):
        """ä¿å­˜æ¨¡å‹å’Œçµæœ"""
        print("\n" + "=" * 70)
        print("ğŸ’¾ STEP 7: æ¨¡å‹ä¿å­˜")
        print("=" * 70)
        
        joblib.dump(self.model, 'phishing_logistic_model.pkl')
        joblib.dump(self.scaler, 'phishing_scaler.pkl')
        joblib.dump(self.metrics, 'phishing_metrics.pkl')
        
        print(f"\nâœ“ å·²ä¿å­˜:")
        print(f"  - phishing_logistic_model.pkl")
        print(f"  - phishing_scaler.pkl")
        print(f"  - phishing_metrics.pkl")
    
    def run_pipeline(self):
        """åŸ·è¡Œå®Œæ•´ Pipeline"""
        print("\n" + "ğŸ¯ é‡£é­šç¶²ç«™åµæ¸¬ - Logistic Regression Pipeline\n")
        
        self.load_data()
        self.explore_data()
        self.preprocess_data()
        self.split_data()
        self.train_model()
        self.evaluate_model()
        self.save_model()
        
        print("\n" + "=" * 70)
        print("âœ… è¨“ç·´å®Œæˆï¼")
        print("=" * 70 + "\n")

if __name__ == "__main__":
    detector = PhishingDetector()
    detector.run_pipeline()
