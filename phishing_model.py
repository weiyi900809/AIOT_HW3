import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, roc_auc_score, roc_curve, classification_report
)
import pickle
import os
from pathlib import Path

class PhishingDetector:
    """
    é‡£é­šéƒµä»¶æª¢æ¸¬æ¨¡å‹é¡åˆ¥
    åŒ…å«è³‡æ–™è¼‰å…¥ã€å‰è™•ç†ã€è¨“ç·´ã€é æ¸¬ç­‰åŠŸèƒ½
    """
    
    def __init__(self, model_path='models/phishing_model.pkl'):
        """åˆå§‹åŒ–æ¨¡å‹"""
        self.model = None
        self.scaler = None
        self.model_path = model_path
        self.metrics = {}
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        
        # å»ºç«‹æ¨¡å‹è³‡æ–™å¤¾
        Path(os.path.dirname(model_path)).mkdir(parents=True, exist_ok=True)
    
    def load_data(self, filepath):
        """
        è¼‰å…¥è³‡æ–™é›†
        
        Args:
            filepath: CSV æª”æ¡ˆè·¯å¾‘
        
        Returns:
            tuple: (ç‰¹å¾µ, æ¨™ç±¤)
        """
        print("ğŸ“‚ è¼‰å…¥è³‡æ–™é›†...")
        data = np.genfromtxt(filepath, delimiter=',', dtype=np.int32)
        samples = data[:, :-1]
        targets = data[:, -1]
        
        print(f"âœ“ è³‡æ–™é›†å¤§å°: {samples.shape[0]} å€‹æ¨£æœ¬, {samples.shape[1]} å€‹ç‰¹å¾µ")
        print(f"âœ“ æ¨™ç±¤åˆ†å¸ƒ: {np.unique(targets, return_counts=True)}")
        
        return samples, targets
    
    def check_data_quality(self, X, y):
        """
        æª¢æŸ¥è³‡æ–™å“è³ª
        
        Args:
            X: ç‰¹å¾µ
            y: æ¨™ç±¤
        """
        print("\\nğŸ” è³‡æ–™å“è³ªæª¢æŸ¥...")
        
        # æª¢æŸ¥ç¼ºå¤±å€¼
        missing_features = np.isnan(X).sum()
        print(f"âœ“ ç¼ºå¤±å€¼: {missing_features}")
        
        # æª¢æŸ¥ç•°å¸¸å€¼ï¼ˆè¶…å‡º [-1, 1] ç¯„åœï¼‰
        invalid_values = np.sum((X < -1) | (X > 1))
        print(f"âœ“ ç•°å¸¸å€¼: {invalid_values}")
        
        # æª¢æŸ¥æ¨™ç±¤åˆ†å¸ƒ
        unique_labels, counts = np.unique(y, return_counts=True)
        print(f"âœ“ æ¨™ç±¤åˆ†å¸ƒ:")
        for label, count in zip(unique_labels, counts):
            print(f"  - æ¨™ç±¤ {label}: {count} ({count/len(y)*100:.2f}%)")
        
        # æª¢æŸ¥é¡åˆ¥ä¸å¹³è¡¡
        min_class = counts.min()
        max_class = counts.max()
        imbalance_ratio = max_class / min_class
        print(f"âœ“ é¡åˆ¥ä¸å¹³è¡¡æ¯”ä¾‹: {imbalance_ratio:.2f}:1")
        
        return {
            'missing': missing_features,
            'invalid': invalid_values,
            'imbalance_ratio': imbalance_ratio
        }
    
    def preprocess_data(self, X, y, test_size=0.2, random_state=42):
        """
        å‰è™•ç†è³‡æ–™
        
        Args:
            X: ç‰¹å¾µ
            y: æ¨™ç±¤
            test_size: æ¸¬è©¦é›†æ¯”ä¾‹
            random_state: éš¨æ©Ÿç¨®å­
        """
        print("\\nâš™ï¸  è³‡æ–™å‰è™•ç†...")
        
        # 1. è³‡æ–™åˆ†å‰²ï¼ˆä½¿ç”¨åˆ†å±¤æŠ½æ¨£ä¿æŒæ¨™ç±¤åˆ†å¸ƒï¼‰
        print("  1ï¸âƒ£  åˆ†å‰²è¨“ç·´/æ¸¬è©¦é›†...")
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=random_state, 
            stratify=y  # ä¿æŒæ¨™ç±¤æ¯”ä¾‹
        )
        print(f"     âœ“ è¨“ç·´é›†: {X_train.shape[0]} å€‹æ¨£æœ¬")
        print(f"     âœ“ æ¸¬è©¦é›†: {X_test.shape[0]} å€‹æ¨£æœ¬")
        
        # 2. ç‰¹å¾µç¸®æ”¾
        print("  2ï¸âƒ£  ç‰¹å¾µæ¨™æº–åŒ–...")
        self.scaler = StandardScaler()
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        print(f"     âœ“ è¨“ç·´é›† - å‡å€¼: {X_train_scaled.mean():.4f}, æ¨™æº–å·®: {X_train_scaled.std():.4f}")
        print(f"     âœ“ æ¸¬è©¦é›† - å‡å€¼: {X_test_scaled.mean():.4f}, æ¨™æº–å·®: {X_test_scaled.std():.4f}")
        
        # 3. æª¢æ¸¬ç•°å¸¸å€¼
        print("  3ï¸âƒ£  ç•°å¸¸å€¼æª¢æ¸¬...")
        outliers = np.sum(np.abs(X_train_scaled) > 3)  # 3-sigma rule
        print(f"     âœ“ ç™¼ç¾ {outliers} å€‹æ½›åœ¨ç•°å¸¸å€¼")
        
        self.X_train = X_train_scaled
        self.X_test = X_test_scaled
        self.y_train = y_train
        self.y_test = y_test
        
        return X_train_scaled, X_test_scaled, y_train, y_test
    
    def train(self, X_train, y_train, cv=5):
        \"\"\"
        è¨“ç·´æ¨¡å‹
        
        Args:
            X_train: è¨“ç·´ç‰¹å¾µ
            y_train: è¨“ç·´æ¨™ç±¤
            cv: äº¤å‰é©—è­‰æ‘ºæ•¸
        \"\"\"
        print("\\nğŸš€ æ¨¡å‹è¨“ç·´...")
        
        # å»ºç«‹æ¨¡å‹
        self.model = LogisticRegression(
            max_iter=1000,
            random_state=42,
            class_weight='balanced'  # è™•ç†é¡åˆ¥ä¸å¹³è¡¡
        )
        
        # è¨“ç·´
        print("  è¨“ç·´ä¸­...")
        self.model.fit(X_train, y_train)
        
        # äº¤å‰é©—è­‰
        print(f"  é€²è¡Œ {cv} æŠ˜äº¤å‰é©—è­‰...")
        cv_scores = cross_val_score(
            self.model, X_train, y_train,
            cv=StratifiedKFold(n_splits=cv, shuffle=True, random_state=42),
            scoring='f1'
        )
        
        print(f"âœ“ è¨“ç·´å®Œæˆ")
        print(f"  äº¤å‰é©—è­‰ F1 åˆ†æ•¸: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")
        
        self.metrics['cv_scores'] = cv_scores
    
    def evaluate(self, X_test, y_test):
        \"\"\"
        è©•ä¼°æ¨¡å‹æ€§èƒ½
        
        Args:
            X_test: æ¸¬è©¦ç‰¹å¾µ
            y_test: æ¸¬è©¦æ¨™ç±¤
        
        Returns:
            dict: è©•ä¼°æŒ‡æ¨™
        \"\"\"
        print("\\nğŸ“Š æ¨¡å‹è©•ä¼°...")
        
        # é æ¸¬
        y_pred = self.model.predict(X_test)
        y_pred_proba = self.model.predict_proba(X_test)[:, 1]
        
        # è¨ˆç®—æŒ‡æ¨™
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        roc_auc = roc_auc_score(y_test, y_pred_proba)
        
        # æ··æ·†çŸ©é™£
        cm = confusion_matrix(y_test, y_pred)
        
        # è©³ç´°å ±å‘Š
        report = classification_report(y_test, y_pred)
        
        # å„²å­˜æŒ‡æ¨™
        self.metrics['accuracy'] = accuracy
        self.metrics['precision'] = precision
        self.metrics['recall'] = recall
        self.metrics['f1'] = f1
        self.metrics['roc_auc'] = roc_auc
        self.metrics['confusion_matrix'] = cm
        self.metrics['y_test'] = y_test
        self.metrics['y_pred'] = y_pred
        self.metrics['y_pred_proba'] = y_pred_proba
        
        print(f"âœ“ æº–ç¢ºåº¦ (Accuracy):   {accuracy:.4f} ({accuracy*100:.2f}%)")
        print(f"âœ“ ç²¾åº¦ (Precision):   {precision:.4f}")
        print(f"âœ“ å¬å›ç‡ (Recall):    {recall:.4f}")
        print(f"âœ“ F1 åˆ†æ•¸:            {f1:.4f}")
        print(f"âœ“ ROC-AUC åˆ†æ•¸:       {roc_auc:.4f}")
        print(f"\\nè©³ç´°åˆ†é¡å ±å‘Š:\\n{report}")
        
        return self.metrics
    
    def predict(self, X):
        \"\"\"
        é€²è¡Œé æ¸¬
        
        Args:
            X: è¼¸å…¥ç‰¹å¾µ
        
        Returns:
            tuple: (é æ¸¬æ¨™ç±¤, é æ¸¬æ©Ÿç‡)
        \"\"\"
        if self.model is None:
            raise ValueError("æ¨¡å‹å°šæœªè¨“ç·´ï¼Œè«‹å…ˆè¨“ç·´æ¨¡å‹")
        
        X_scaled = self.scaler.transform(X)
        predictions = self.model.predict(X_scaled)
        probabilities = self.model.predict_proba(X_scaled)
        
        return predictions, probabilities
    
    def save_model(self):
        \"\"\"ä¿å­˜æ¨¡å‹\"\"\"
        if self.model is None:
            raise ValueError("æ²’æœ‰æ¨¡å‹å¯ä»¥ä¿å­˜")
        
        with open(self.model_path, 'wb') as f:
            pickle.dump({
                'model': self.model,
                'scaler': self.scaler,
                'metrics': self.metrics
            }, f)
        
        print(f"\\nğŸ’¾ æ¨¡å‹å·²ä¿å­˜è‡³: {self.model_path}")
    
    def load_model(self):
        \"\"\"è¼‰å…¥å·²ä¿å­˜çš„æ¨¡å‹\"\"\"
        if not os.path.exists(self.model_path):
            raise FileNotFoundError(f"æ¨¡å‹æª”æ¡ˆä¸å­˜åœ¨: {self.model_path}")
        
        with open(self.model_path, 'rb') as f:
            data = pickle.load(f)
            self.model = data['model']
            self.scaler = data['scaler']
            self.metrics = data['metrics']
        
        print(f"âœ“ æ¨¡å‹å·²è¼‰å…¥")
