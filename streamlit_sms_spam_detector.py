# SMS Spam Detector with Streamlit UI - ËºïÈáèÂåñÁâàÊú¨ÔºàÁÑ° wordcloudÔºâ
"""
‰ΩøÁî®ÊñπÂºèÔºö
Âü∑Ë°åÊôÇÊñºÂëΩ‰ª§ÂàóËº∏ÂÖ•
    streamlit run streamlit_sms_spam_detector.py
‰∏¶Â∞á sms_spam_no_header.csv ÊîæÂú®ÂêåÁõÆÈåÑ‰∏ã
"""

import streamlit as st
import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import (
    accuracy_score, classification_report, confusion_matrix, roc_curve, auc
)
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# Ë®≠ÂÆö Streamlit È†ÅÈù¢
st.set_page_config(page_title="SMS Spam Detector", layout="wide", initial_sidebar_state="expanded")

# Ë®≠ÂÆöÂúñË°®È¢®Ê†º
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

st.title("üìß SMS Spam Detector")
st.markdown("**Logistic Regression ÂûÉÂúæÈÉµ‰ª∂ÂÅµÊ∏¨Á≥ªÁµ±**")

# ===============================
# 1. Ë≥áÊñôËºâÂÖ•Ôºà‰ΩøÁî® cache Âä†ÈÄüÔºâ
# ===============================

@st.cache_data(show_spinner=True)
def load_data():
    """ËºâÂÖ•‰∏¶È†êËôïÁêÜË≥áÊñôÈõÜ"""
    try:
        df = pd.read_csv('sms_spam_no_header.csv', header=None, names=['label', 'message'], encoding='latin-1')
        df['label_num'] = df['label'].map({'ham': 0, 'spam': 1})
        df['message_length'] = df['message'].str.len()
        df['word_count'] = df['message'].str.split().str.len()
        return df
    except FileNotFoundError:
        st.error("‚ùå Êâæ‰∏çÂà∞ sms_spam_no_header.csv Ê™îÊ°àÔºÅË´ãÁ¢∫‰øùÊ≠§Ê™îÊ°àÂú®Âêå‰∏ÄÁõÆÈåÑ‰∏ã„ÄÇ")
        st.stop()
    except Exception as e:
        st.error(f"‚ùå Ë≥áÊñôËºâÂÖ•Â§±Êïó: {str(e)}")
        st.stop()

df = load_data()
st.success("‚úÖ Ë≥áÊñôËºâÂÖ•ÊàêÂäü")

# ===============================
# 2. ÂÅ¥ÈÇäÊ¨Ñ - Ë≥áÊñôÊé¢Á¥¢ÂçÄ
# ===============================

with st.sidebar:
    st.header("üìä Data Overview")
    
    # Âü∫Êú¨Áµ±Ë®à
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Total", df.shape[0])
    with col2:
        st.metric("Ham", sum(df['label'] == 'ham'))
    with col3:
        st.metric("Spam", sum(df['label'] == 'spam'))
    
    st.divider()
    
    # È°ûÂà•ÂàÜÂ∏É
    st.subheader("üìà Label Distribution")
    fig1, ax1 = plt.subplots(figsize=(5, 3))
    label_counts = df['label'].value_counts()
    colors_pie = ['#3498db', '#e74c3c']
    wedges, texts, autotexts = ax1.pie(label_counts.values, labels=label_counts.index, 
                                         autopct='%1.1f%%', colors=colors_pie, startangle=90)
    for autotext in autotexts:
        autotext.set_color('white')
        autotext.set_fontweight('bold')
    ax1.set_title('Ham vs Spam', fontsize=11, fontweight='bold')
    st.pyplot(fig1, use_container_width=True)
    
    st.divider()
    
    # Ë®äÊÅØÈï∑Â∫¶Áµ±Ë®à
    st.subheader("üìè Message Length Stats")
    stats = df.groupby('label')['message_length'].describe()[['mean', '50%', 'max']]
    st.dataframe(stats.round(0), use_container_width=True)
    
    # Ë®äÊÅØÈï∑Â∫¶ÂàÜÂ∏ÉÂúñ
    fig2, ax2 = plt.subplots(figsize=(5, 3))
    ax2.hist(df[df['label'] == 'ham']['message_length'], bins=40, alpha=0.6, 
            label='Ham', color='#3498db', density=True)
    ax2.hist(df[df['label'] == 'spam']['message_length'], bins=40, alpha=0.6, 
            label='Spam', color='#e74c3c', density=True)
    ax2.set_xlabel('Message Length', fontsize=9)
    ax2.set_ylabel('Density', fontsize=9)
    ax2.legend(fontsize=9)
    ax2.set_title('Length Distribution', fontsize=10, fontweight='bold')
    st.pyplot(fig2, use_container_width=True)
    
    st.divider()
    
    # Ê®£Êú¨È†êË¶Ω
    st.subheader("üìù Sample Messages")
    sample_idx = st.slider("Select sample index", 0, min(20, len(df)-1), 0)
    sample = df.iloc[sample_idx]
    col_label, col_msg = st.columns([1, 3])
    with col_label:
        st.metric("Label", sample['label'].upper())
    with col_msg:
        st.write(f"**Message:** {sample['message'][:100]}...")

# ===============================
# 3. Ê®°ÂûãË®ìÁ∑¥Ôºà‰ΩøÁî® cache Âä†ÈÄüÔºâ
# ===============================

@st.cache_resource(show_spinner=True)
def train_model(df):
    """Ë®ìÁ∑¥ Logistic Regression Ê®°Âûã"""
    X = df['message']
    y = df['label_num']
    
    # Ë≥áÊñôÂàÜÂâ≤
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, stratify=y, random_state=42)
    
    # ÁâπÂæµÊèêÂèñ
    vectorizer = TfidfVectorizer(max_features=3000, stop_words='english', 
                                max_df=0.95, min_df=2, ngram_range=(1, 2))
    X_train_tfidf = vectorizer.fit_transform(X_train)
    X_test_tfidf = vectorizer.transform(X_test)
    
    # Ê®°ÂûãË®ìÁ∑¥
    model = LogisticRegression(max_iter=1000, random_state=42, solver='lbfgs', C=1.0)
    model.fit(X_train_tfidf, y_train)
    
    return model, vectorizer, X_test, y_test, X_test_tfidf

# Ë®ìÁ∑¥Ê®°Âûã
with st.spinner("ü§ñ Ë®ìÁ∑¥Ê®°Âûã‰∏≠..."):
    model, vectorizer, X_test, y_test, X_test_tfidf = train_model(df)

# ===============================
# 4. ‰∏ªÈ†Å - Ê®°ÂûãÊïàËÉΩÂ±ïÁ§∫
# ===============================

st.header("üéØ Model Performance")

# È†êÊ∏¨
y_pred = model.predict(X_test_tfidf)
y_proba = model.predict_proba(X_test_tfidf)[:, 1]
acc = accuracy_score(y_test, y_pred)

# Ë®àÁÆóÊåáÊ®ô
cm = confusion_matrix(y_test, y_pred)
tn, fp, fn, tp = cm[0][0], cm[0][1], cm[1][0], cm[1][1]
specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
precision = tp / (tp + fp) if (tp + fp) > 0 else 0
fpr, tpr, _ = roc_curve(y_test, y_proba)
roc_auc = auc(fpr, tpr)

# ÊïàËÉΩÊåáÊ®ôÂç°Áâá
col_m1, col_m2, col_m3, col_m4, col_m5 = st.columns(5)
with col_m1:
    st.metric("Accuracy", f"{acc*100:.1f}%", delta="üìä")
with col_m2:
    st.metric("Sensitivity", f"{sensitivity*100:.1f}%", delta="üìà")
with col_m3:
    st.metric("Specificity", f"{specificity*100:.1f}%", delta="üìà")
with col_m4:
    st.metric("Precision", f"{precision*100:.1f}%", delta="üìà")
with col_m5:
    st.metric("ROC AUC", f"{roc_auc:.3f}", delta="üéØ")

st.divider()

# Ë¶ñË¶∫ÂåñÂúñË°®
col_chart1, col_chart2, col_chart3 = st.columns(3)

with col_chart1:
    st.subheader("Confusion Matrix")
    fig_cm, ax_cm = plt.subplots(figsize=(5, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'], 
                ax=ax_cm, cbar_kws={'label': 'Count'}, annot_kws={'size': 14})
    ax_cm.set_xlabel('Predicted Label', fontsize=10, fontweight='bold')
    ax_cm.set_ylabel('True Label', fontsize=10, fontweight='bold')
    st.pyplot(fig_cm, use_container_width=True)

with col_chart2:
    st.subheader("ROC Curve")
    fig_roc, ax_roc = plt.subplots(figsize=(5, 4))
    ax_roc.plot(fpr, tpr, color='#e74c3c', lw=3, label=f'AUC = {roc_auc:.3f}')
    ax_roc.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', label='Random')
    ax_roc.fill_between(fpr, tpr, alpha=0.3, color='#e74c3c')
    ax_roc.set_xlabel('False Positive Rate', fontsize=10, fontweight='bold')
    ax_roc.set_ylabel('True Positive Rate', fontsize=10, fontweight='bold')
    ax_roc.legend(loc='lower right', fontsize=10)
    ax_roc.grid(alpha=0.3)
    st.pyplot(fig_roc, use_container_width=True)

with col_chart3:
    st.subheader("Test Set Distribution")
    fig_test, ax_test = plt.subplots(figsize=(5, 4))
    pred_dist = pd.Series(y_pred).value_counts()
    colors_test = ['#3498db', '#e74c3c']
    ax_test.bar(['Predicted Ham', 'Predicted Spam'], 
               [pred_dist.get(0, 0), pred_dist.get(1, 0)],
               color=colors_test, alpha=0.7, edgecolor='black', linewidth=2)
    ax_test.set_ylabel('Count', fontsize=10, fontweight='bold')
    ax_test.set_title('Test Set Predictions', fontsize=11, fontweight='bold')
    for i, v in enumerate([pred_dist.get(0, 0), pred_dist.get(1, 0)]):
        ax_test.text(i, v + 5, str(v), ha='center', fontweight='bold')
    st.pyplot(fig_test, use_container_width=True)

st.divider()

# Ë©≥Á¥∞ÂàÜÈ°ûÂ†±Âëä
st.subheader("üìã Classification Report")
report_dict = classification_report(y_test, y_pred, target_names=['Ham', 'Spam'], 
                                    output_dict=True)
report_df = pd.DataFrame(report_dict).T
st.dataframe(report_df.round(3), use_container_width=True)

# ===============================
# 5. ‰∫íÂãïÂºèÈ†êÊ∏¨
# ===============================

st.header("üí¨ Test Your Message")
st.write("Ëº∏ÂÖ•‰∏ÄÂâá SMS Ë®äÊÅØÔºåÁ≥ªÁµ±ÊúÉËá™ÂãïÂà§Êñ∑ÊòØ HamÔºàÊ≠£Â∏∏Ë®äÊÅØÔºâÈÇÑÊòØ SpamÔºàÂûÉÂúæË®äÊÅØÔºâ")

user_input = st.text_area(
    "Enter an SMS message:",
    placeholder="e.g., Congratulations! You've won a prize. Call now!",
    height=80,
    key="user_message"
)

if user_input and len(user_input.strip()) > 0:
    msg_tfidf = vectorizer.transform([user_input])
    pred = model.predict(msg_tfidf)[0]
    proba = model.predict_proba(msg_tfidf)[0]
    
    # È°ØÁ§∫È†êÊ∏¨ÁµêÊûú
    col_res1, col_res2, col_res3 = st.columns([2, 1, 1])
    
    with col_res1:
        if pred == 1:
            st.error("üî¥ **SPAM** - This message is likely spam!")
        else:
            st.success("üü¢ **HAM** - This message appears to be legitimate!")
    
    with col_res2:
        st.metric("Spam Score", f"{proba[1]*100:.1f}%")
    
    with col_res3:
        st.metric("Ham Score", f"{proba[0]*100:.1f}%")
    
    # ‰ø°ÂøÉÂ∫¶ÊåáÁ§∫
    st.write("**Confidence Level:**")
    max_prob = max(proba)
    col_conf = st.columns([int(max_prob * 100), 100 - int(max_prob * 100)])
    with col_conf[0]:
        st.success(f"{'‚ñà' * int(max_prob * 20)}")
    st.write(f"‰ø°ÂøÉÂ∫¶: {max_prob*100:.1f}%")

# ===============================
# 6. ÁâπÂæµÈáçË¶ÅÊÄß
# ===============================

st.header("üîç Top Important Keywords")
st.write("ÈÄô‰∫õÈóúÈçµÂ≠óÂ∞çÊ®°ÂûãÂà§Êñ∑ÊòØÂê¶ÁÇ∫ Spam ÊúÄÊúâÂΩ±ÈüøÂäõ")

features = vectorizer.get_feature_names_out()
importances = np.abs(model.coef_[0])
top_indices = np.argsort(importances)[-12:][::-1]

fig_imp, ax_imp = plt.subplots(figsize=(8, 5))
colors_imp = ['#e74c3c' if model.coef_[0][i] > 0 else '#3498db' for i in top_indices]
bars = ax_imp.barh(range(len(top_indices)), importances[top_indices], color=colors_imp, alpha=0.7, edgecolor='black')
ax_imp.set_yticks(range(len(top_indices)))
ax_imp.set_yticklabels(features[top_indices], fontsize=10)
ax_imp.set_xlabel('Importance Score', fontsize=10, fontweight='bold')
ax_imp.set_title('Top 12 Important Keywords', fontsize=12, fontweight='bold')
ax_imp.invert_yaxis()
st.pyplot(fig_imp, use_container_width=True)

# È†ÅÂ∞æ
st.divider()
col_footer1, col_footer2 = st.columns([3, 1])
with col_footer1:
    st.write("**Model**: Logistic Regression | **Algorithm**: TF-IDF Vectorization | **Dataset**: SMS Spam Collection")
with col_footer2:
    st.write("‚úÖ App Status: Online")
