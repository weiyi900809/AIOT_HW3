# SMS Spam Detector with Streamlit UI
"""
ä½¿ç”¨æ–¹å¼ï¼š
åŸ·è¡Œæ™‚æ–¼å‘½ä»¤åˆ—è¼¸å…¥
    streamlit run streamlit_sms_spam_detector.py
ä¸¦å°‡ sms_spam_no_header.csv æ”¾åœ¨åŒç›®éŒ„ä¸‹
"""

import streamlit as st
import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import (
    accuracy_score, classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve
)
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import io

st.set_page_config(page_title="SMS Spam Detector", layout="wide")

st.title("ğŸ“§ SMS Spam Detector åƒåœ¾éƒµä»¶åµæ¸¬ (Logistic Regression)")

@st.cache_data(show_spinner=True)
def load_data():
    df = pd.read_csv('sms_spam_no_header.csv', header=None, names=['label', 'message'])
    df['label_num'] = df['label'].map({'ham': 0, 'spam': 1})
    df['message_length'] = df['message'].str.len()
    df['word_count'] = df['message'].str.split().str.len()
    return df

df = load_data()

# è³‡æ–™æ¢ç´¢å€
with st.sidebar:
    st.header('ğŸ” è³‡æ–™æ¢ç´¢')
    st.dataframe(df.head(10))
    st.write(f"æ¨£æœ¬æ•¸: {df.shape[0]}, Ham: {sum(df['label']=='ham')}, Spam: {sum(df['label']=='spam')}")

    st.write('---')
    fig1, ax1 = plt.subplots()
    ax1.pie(df['label'].value_counts(), labels=['ham', 'spam'], autopct="%.1f%%", colors=['skyblue', 'salmon'])
    st.pyplot(fig1)
    st.write('é¡åˆ¥åˆ†å¸ƒ')

    st.write('---')
    st.write('è¨Šæ¯é•·åº¦åˆ†å¸ƒ')
    fig2, ax2 = plt.subplots()
    for label, color in [('ham','blue'),('spam','red')]:
        ax2.hist(df[df['label'] == label]['message_length'], bins=50, alpha=0.7, label=label, color=color, density=True)
    ax2.legend(); ax2.set_xlabel('è¨Šæ¯é•·åº¦'); ax2.set_ylabel('å¯†åº¦')
    st.pyplot(fig2)

    st.write('---')
    st.write('Ham & Spam æ–‡å­—é›²')
    ham_text = ' '.join(df[df['label']=='ham']['message'])
    spam_text = ' '.join(df[df['label']=='spam']['message'])
    col1, col2 = st.columns(2)
    with col1:
        st.write('Ham')
        wc1 = WordCloud(width=250, height=180, background_color='white').generate(ham_text)
        st.image(wc1.to_array())
    with col2:
        st.write('Spam')
        wc2 = WordCloud(width=250, height=180, background_color='white').generate(spam_text)
        st.image(wc2.to_array())

# è¨“ç·´èˆ‡æ¸¬è©¦æ¨¡å‹
@st.cache_resource(show_spinner=True)
def train_model(df):
    X = df['message']
    y = df['label_num']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)
    vectorizer = TfidfVectorizer(max_features=3000, stop_words='english')
    X_train_tfidf = vectorizer.fit_transform(X_train)
    X_test_tfidf = vectorizer.transform(X_test)
    model = LogisticRegression(max_iter=1000, random_state=42)
    model.fit(X_train_tfidf, y_train)
    return model, vectorizer, X_test, y_test, X_test_tfidf

model, vectorizer, X_test, y_test, X_test_tfidf = train_model(df)
y_pred = model.predict(X_test_tfidf)
y_proba = model.predict_proba(X_test_tfidf)[:,1]
acc = accuracy_score(y_test, y_pred)

# ä¸»é çµæœ
st.subheader('ğŸ¯ æ¨¡å‹æ•ˆèƒ½')
st.write(f'**æ¸¬è©¦é›†æº–ç¢ºç‡:** {acc*100:.2f}%')

colA, colB, colC = st.columns(3)
with colA:
    st.metric("Ham æº–ç¢ºç‡", f"{accuracy_score(y_test[y_test==0], y_pred[y_test==0])*100:.2f}%")
with colB:
    st.metric("Spam æº–ç¢ºç‡", f"{accuracy_score(y_test[y_test==1], y_pred[y_test==1])*100:.2f}%")
with colC:
    st.metric("AUC åˆ†æ•¸", f"{auc(*roc_curve(y_test, y_proba)[:2]):.3f}")

report = classification_report(y_test, y_pred, target_names=['Ham', 'Spam'], output_dict=True)
st.write('**è©³ç´°åˆ†é¡å ±å‘Š:**')
st.dataframe(pd.DataFrame(report).T)

cm = confusion_matrix(y_test, y_pred)
fig_cm, ax_cm = plt.subplots()
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'], ax=ax_cm)
ax_cm.set_xlabel('é æ¸¬'); ax_cm.set_ylabel('å¯¦éš›'); ax_cm.set_title('æ··æ·†çŸ©é™£')
st.pyplot(fig_cm)

# ROC æ›²ç·š
fpr, tpr, _ = roc_curve(y_test, y_proba)
fig_roc, ax_roc = plt.subplots()
ax_roc.plot(fpr, tpr, label=f'AUC={auc(fpr,tpr):.3f}', color='orange')
ax_roc.plot([0,1],[0,1],'--',color='grey')
ax_roc.set_xlabel('å‡é™½æ€§ç‡(FPR)')
ax_roc.set_ylabel('çœŸé™½æ€§ç‡(TPR)')
ax_roc.legend()
ax_roc.set_title('ROC æ›²ç·š')
st.pyplot(fig_roc)

# ç”¨æˆ¶äº’å‹•é æ¸¬
st.subheader("ğŸ“ è©¦è©¦çœ‹ä½ çš„è¨Šæ¯!")
user_input = st.text_area("è«‹è¼¸å…¥ä¸€å‰‡ SMS å…§å®¹ï¼š", "Congratulations! You've won a free prize. Call now!")
if user_input:
    arr = vectorizer.transform([user_input])
    pre = model.predict(arr)[0]
    proba = model.predict_proba(arr)[0]
    st.write(f'é æ¸¬çµæœï¼š**{"Spam" if pre==1 else "Ham"}** (Spam æ©Ÿç‡: {proba[1]:.2%}, Ham æ©Ÿç‡: {proba[0]:.2%})')

# ç‰¹å¾µé‡è¦æ€§
st.subheader("ğŸ” é‡è¦ç‰¹å¾µåˆ†æ")
features = vectorizer.get_feature_names_out()
importances = np.abs(model.coef_[0])
idx = np.argsort(importances)[-20:][::-1]
fig_imp, ax_imp = plt.subplots(figsize=(6,5))
sns.barplot(y=features[idx], x=importances[idx], palette='viridis', ax=ax_imp)
ax_imp.set_title('Top 20 æ–‡å­—ç‰¹å¾µå½±éŸ¿åŠ›')
st.pyplot(fig_imp)
